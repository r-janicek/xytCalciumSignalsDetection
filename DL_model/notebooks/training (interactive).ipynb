{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net Model Training Script\n",
    "\n",
    "**Author:** Prisca Dotti\n",
    "\n",
    "**Last Edit:** 24.10.2023\n",
    "\n",
    "This Jupyter Notebook contains the code for training a U-Net model on a dataset of sparks videos. The dataset is split into training and testing sets, and the model is trained using the training set. The testing set is used to evaluate the performance of the trained model.\n",
    "\n",
    "To run the notebook, simply execute each cell in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload is used to reload modules automatically before entering the\n",
    "# execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# To import modules from parent directory in Jupyter Notebook\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "# from torch.cuda.amp import GradScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "import wandb\n",
    "from config import TrainingConfig, config\n",
    "from models.UNet import unet\n",
    "from utils.training_inference_tools import (\n",
    "    MyTrainingManager,\n",
    "    sampler,\n",
    "    test_function,\n",
    "    training_step,\n",
    "    weights_init,\n",
    ")\n",
    "from utils.training_script_utils import (\n",
    "    get_sample_ids,\n",
    "    init_criterion,\n",
    "    init_dataset,\n",
    "    init_model,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:17:58] [  INFO  ] [   config   ] <290 > -- Loading C:\\Users\\prisc\\Code\\sparks_project\\config_files\\config_final_model.ini\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --     training_config_file: C:\\Users\\prisc\\Code\\sparks_project\\config_files\\config_final_model.ini\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --              dataset_dir: C:\\Users\\prisc\\Code\\sparks_project\\data\\sparks_dataset\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --                        c: <configparser.ConfigParser object at 0x000002A12C5FCD90>\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --                 run_name: final_model\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --            load_run_name: \n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --               load_epoch: 100000\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --             train_epochs: 90000\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --                criterion: lovasz_softmax\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --                 lr_start: 0.0001\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --       ignore_frames_loss: 6\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --                     cuda: True\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --                scheduler: None\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --                optimizer: adam\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --             dataset_size: minimal\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --               batch_size: 4\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --              num_workers: 0\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --            data_duration: 256\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --              data_stride: 32\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --           data_smoothing: no\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --               norm_video: abs_max\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --        remove_background: no\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --  noise_data_augmentation: False\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --              sparks_type: raw\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --                  new_fps: 0\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --  inference_data_duration: 256\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --    inference_data_stride: 32\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --                inference: overlap\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --     inference_load_epoch: 100000\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --     inference_batch_size: 4\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --   inference_dataset_size: full\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --          nn_architecture: pablos_unet\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --               unet_steps: 6\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --     first_layer_channels: 8\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --             num_channels: 1\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --                 dilation: 1\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --              border_mode: same\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --      batch_normalization: none\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --       temporal_reduction: False\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --       initialize_weights: True\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --                wandb_log: False\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --                   device: cuda\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --               pin_memory: True\n",
      "[16:17:59] [  INFO  ] [   config   ] <532 > --                   n_gpus: 1\n"
     ]
    }
   ],
   "source": [
    "##################### Get training-specific parameters #####################\n",
    "\n",
    "# Initialize training-specific parameters\n",
    "# (get the configuration file path from ArgParse)\n",
    "config_filename = os.path.join(\"config_files\", \"config_final_model.ini\")\n",
    "params = TrainingConfig(training_config_file=config_filename)\n",
    "\n",
    "# Print parameters to console if needed\n",
    "params.print_params()\n",
    "\n",
    "######################### Initialize random seeds ##########################\n",
    "\n",
    "# We used these random seeds to ensure reproducibility of the results\n",
    "\n",
    "# torch.manual_seed(0) <--------------------------------------------------!\n",
    "# random.seed(0) <--------------------------------------------------------!\n",
    "# np.random.seed(0) <-----------------------------------------------------!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:17:59] [  INFO  ] [   config   ] <528 > -- Using cpu\n"
     ]
    }
   ],
   "source": [
    "# params.set_device(\"cpu\")\n",
    "# params.display_device_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:17:59] [  INFO  ] [utils.training_script_utils] <137 > -- Samples in training dataset: 9\n",
      "[16:17:59] [WARNING ] [tifffile.tifffile] <16549> -- TiffPage 0: TypeError: read_bytes() missing 3 required positional arguments: 'dtype', 'count', and 'offsetsize'\n",
      "[16:17:59] [WARNING ] [tifffile.tifffile] <16549> -- TiffPage 0: TypeError: read_bytes() missing 3 required positional arguments: 'dtype', 'count', and 'offsetsize'\n",
      "[16:18:03] [  INFO  ] [utils.training_script_utils] <137 > -- Samples in training dataset: 22\n"
     ]
    }
   ],
   "source": [
    "############################ Configure datasets ############################\n",
    "\n",
    "# Select samples for training and testing based on dataset size\n",
    "train_sample_ids = get_sample_ids(\n",
    "    train_data=True,\n",
    "    dataset_size=params.dataset_size,\n",
    ")\n",
    "test_sample_ids = get_sample_ids(\n",
    "    train_data=False,\n",
    "    dataset_size=params.dataset_size,\n",
    ")\n",
    "\n",
    "# Initialize training dataset\n",
    "dataset = init_dataset(\n",
    "    params=params,\n",
    "    sample_ids=train_sample_ids,\n",
    "    apply_data_augmentation=True,\n",
    "    load_instances=False,\n",
    ")\n",
    "\n",
    "# Initialize testing datasets\n",
    "testing_dataset = init_dataset(\n",
    "    params=params,\n",
    "    sample_ids=test_sample_ids,\n",
    "    apply_data_augmentation=False,\n",
    "    load_instances=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train with only one batch\n",
    "# import numpy as np\n",
    "# from torch.utils.data import Subset\n",
    "\n",
    "# ids = list(np.arange(0, params.batch_size, 1, dtype=np.int64))\n",
    "# dataset = Subset(dataset, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loaders\n",
    "dataset_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=params.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=params.num_workers,\n",
    "    pin_memory=params.pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:21:06] [  INFO  ] [  __main__  ] < 17 > -- Initializing UNet weights...\n"
     ]
    }
   ],
   "source": [
    "############################## Configure UNet ##############################\n",
    "\n",
    "# Initialize the UNet model\n",
    "network = init_model(params=params)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "if params.device.type != \"cpu\":\n",
    "    network = nn.DataParallel(network).to(params.device, non_blocking=True)\n",
    "    # cudnn.benchmark = True\n",
    "\n",
    "# Watch the model with wandb for logging if enabled\n",
    "if params.wandb_log:\n",
    "    wandb.watch(network)\n",
    "\n",
    "# Initialize UNet weights if required\n",
    "if params.initialize_weights:\n",
    "    logger.info(\"Initializing UNet weights...\")\n",
    "    network.apply(weights_init)\n",
    "\n",
    "# The following line is commented as it does not work on Windows\n",
    "# torch.compile(network, mode=\"default\", backend=\"inductor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(network.parameters())).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:21:06] [  INFO  ] [  __main__  ] < 24 > -- Output directory: C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\final_model\n",
      "[16:21:06] [  INFO  ] [utils.training_inference_tools] <1338> -- Loading 'C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\final_model\\network_100000.pth'...\n",
      "[16:21:06] [WARNING ] [utils.training_inference_tools] <1344> -- Failed to load the model, as it was trained with DataParallel. Wrapping it in DataParallel and retrying...\n",
      "[16:21:06] [  INFO  ] [utils.training_inference_tools] <1352> -- Network should be on CPU, removing DataParallel wrapper...\n",
      "[16:21:06] [  INFO  ] [utils.training_inference_tools] <1338> -- Loading 'C:\\Users\\prisc\\Code\\sparks_project\\models\\saved_models\\final_model\\optimizer_100000.pth'...\n"
     ]
    }
   ],
   "source": [
    "########################### Initialize training ############################\n",
    "\n",
    "# Initialize the optimizer based on the specified type\n",
    "if params.optimizer == \"adam\":\n",
    "    optimizer = optim.Adam(network.parameters(), lr=params.lr_start)\n",
    "elif params.optimizer == \"adadelta\":\n",
    "    optimizer = optim.Adadelta(network.parameters(), lr=params.lr_start)\n",
    "else:\n",
    "    logger.error(f\"{params.optimizer} is not a valid optimizer.\")\n",
    "    exit()\n",
    "\n",
    "# Initialize the learning rate scheduler if specified\n",
    "if params.scheduler == \"step\":\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer,\n",
    "        step_size=params.scheduler_step_size,\n",
    "        gamma=params.scheduler_gamma,\n",
    "    )\n",
    "else:\n",
    "    scheduler = None\n",
    "\n",
    "# Define the output directory path\n",
    "output_path = os.path.join(config.output_dir, params.run_name)\n",
    "logger.info(f\"Output directory: {os.path.realpath(output_path)}\")\n",
    "\n",
    "# Initialize the summary writer for TensorBoard logging\n",
    "summary_writer = SummaryWriter(os.path.join(output_path, \"summary\"), purge_step=0)\n",
    "\n",
    "# Check if a pre-trained model should be loaded\n",
    "if params.load_run_name != \"\":\n",
    "    load_path = os.path.join(config.output_dir, params.load_run_name)\n",
    "    logger.info(f\"Model loaded from directory: {os.path.realpath(load_path)}\")\n",
    "else:\n",
    "    load_path = None\n",
    "\n",
    "# Initialize the loss function\n",
    "criterion = init_criterion(params=params, dataset=dataset)\n",
    "\n",
    "# Create a directory to save predicted class movies\n",
    "preds_output_dir = os.path.join(output_path, \"predictions\")\n",
    "os.makedirs(preds_output_dir, exist_ok=True)\n",
    "\n",
    "# Create a dictionary of managed objects\n",
    "managed_objects = {\"network\": network, \"optimizer\": optimizer}\n",
    "if scheduler is not None:\n",
    "    managed_objects[\"scheduler\"] = scheduler\n",
    "\n",
    "# Create a training manager with the specified training and testing functions\n",
    "trainer = MyTrainingManager(\n",
    "    # Training parameters\n",
    "    training_step=lambda _: training_step(\n",
    "        dataset_loader=dataset_loader,\n",
    "        params=params,\n",
    "        sampler=sampler,\n",
    "        network=network,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        scheduler=scheduler,\n",
    "        # scaler=GradScaler(),\n",
    "    ),\n",
    "    save_every=params.c.getint(\"training\", \"save_every\", fallback=5000),\n",
    "    load_path=load_path,\n",
    "    save_path=output_path,\n",
    "    managed_objects=unet.managed_objects(managed_objects),\n",
    "    # Testing parameters\n",
    "    test_function=lambda _: test_function(\n",
    "        network=network,\n",
    "        device=params.device,\n",
    "        criterion=criterion,\n",
    "        params=params,\n",
    "        testing_dataset=testing_dataset,\n",
    "        training_name=params.run_name,\n",
    "        output_dir=preds_output_dir,\n",
    "        training_mode=True,\n",
    "        debug=config.debug_mode,\n",
    "    ),\n",
    "    test_every=params.c.getint(\"training\", \"test_every\", fallback=1000),\n",
    "    plot_every=params.c.getint(\"training\", \"test_every\", fallback=1000),\n",
    "    summary_writer=summary_writer,\n",
    ")\n",
    "\n",
    "# Load the model if a specific epoch is provided\n",
    "if params.load_epoch != 0:\n",
    "    trainer.load(params.load_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## Start training ##############################\n",
    "\n",
    "# Resume the W&B run if needed (commented out for now)\n",
    "# if wandb.run.resumed:\n",
    "#     checkpoint = torch.load(wandb.restore(checkpoint_path))\n",
    "#     network.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     epoch = checkpoint['epoch']\n",
    "#     loss = checkpoint['loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:03:12] [  INFO  ] [  __main__  ] < 3  > -- Validate network before training\n",
      "[16:03:12] [  INFO  ] [utils.training_inference_tools] <1173> -- Validating network at iteration 100000...\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1276> -- Metrics:\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \tvalidation_loss: 0.676\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \tsparks/precision: 0.2321\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \tsparks/recall: 0.8125\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \tsparks/correctly_classified: 0.2889\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \tsparks/detected: 0.875\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \twaves/precision: 1\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \twaves/recall: 0.3333\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \twaves/correctly_classified: 1\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \twaves/detected: 0.8333\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \tpuffs/precision: 0.04762\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \tpuffs/recall: 0.3333\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \tpuffs/correctly_classified: 0.09091\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \tpuffs/detected: 0.6667\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \taverage/precision: 0.4266\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \taverage/recall: 0.4931\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \taverage/correctly_classified: 0.4599\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \taverage/detected: 0.7917\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \tsegmentation/sparks_IoU: 0.178\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \tsegmentation/waves_IoU: 0.1328\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \tsegmentation/puffs_IoU: 0.0395\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1282> -- \tsegmentation/average_IoU: 0.1168\n",
      "[16:05:31] [  INFO  ] [utils.training_inference_tools] <1280> -- \tsegmentation_confusion_matrix:\n",
      "[[26558861    10562    19761    42624]\n",
      " [    7142    10081        0     2113]\n",
      " [ 1483773    26622   328679   615876]\n",
      " [   88385      122        0    30809]]\n"
     ]
    }
   ],
   "source": [
    "# Validate the network before training if resuming from a checkpoint\n",
    "if params.load_epoch > 0:\n",
    "    logger.info(\"Validate network before training\")\n",
    "    trainer.run_validation(wandb_log=params.wandb_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:21:20] [  INFO  ] [  __main__  ] < 5  > -- Starting training\n",
      "x device: cpu\n",
      "y device: cpu\n",
      "y_pred device: cpu\n",
      "loss device: cpu\n",
      "[16:23:19] [  INFO  ] [utils.training_inference_tools] <1316> -- Iteration 100000...\n",
      "[16:23:19] [  INFO  ] [utils.training_inference_tools] <1317> -- \tTraining loss: 0.2077\n",
      "[16:23:19] [  INFO  ] [utils.training_inference_tools] <1318> -- \tTime elapsed: 176.47s\n"
     ]
    }
   ],
   "source": [
    "# Set the network in training mode\n",
    "network.train()\n",
    "\n",
    "# Train the model for the specified number of epochs\n",
    "logger.info(\"Starting training\")\n",
    "trainer.train(\n",
    "    params.train_epochs,\n",
    "    print_every=params.c.getint(\"training\", \"print_every\", fallback=100),\n",
    "    wandb_log=params.wandb_log,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Starting final validation\")\n",
    "# Run the final validation/testing procedure\n",
    "trainer.run_validation(wandb_log=params.wandb_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the summary writer\n",
    "summary_writer.close()\n",
    "\n",
    "# Close the wandb run\n",
    "if params.wandb_log:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging purposes\n",
    "# model_parameters = filter(lambda p: p.requires_grad, network.parameters())\n",
    "# model_parameters = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# logger.debug(f\"Number of trainable parameters: {model_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for load_epoch in [10000,20000,30000,40000,50000,60000,70000,80000,90000,100000]:\n",
    "# for load_epoch in [100000]:\n",
    "#     trainer.load(load_epoch)\n",
    "#     logger.info(\"Starting final validation\")\n",
    "#     trainer.run_validation(wandb_log=wandb_log)\n",
    "# if wandb_log:\n",
    "#     wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize UNet architecture (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get number of trainable parameters\n",
    "# num_params = sum(p.numel() for p in network.parameters() if p.requires_grad)\n",
    "# logger.debug(f\"Number of trainable parameters: {num_params}\")\n",
    "# # get dummy unet input\n",
    "# batch = next(iter(dataset_loader))\n",
    "# x = batch[0].to(device)\n",
    "# yhat = network(x[:,None]) # Give dummy batch to forward()\n",
    "# from torchviz import make_dot\n",
    "# make_dot(yhat, params=dict(list(network.named_parameters()))).render(\"unet_model\", format=\"png\")\n",
    "# a = [0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "\n",
    "# len(a[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d33eb8e81965b779f2871c6ab1ae98a760df4ff814358c9a5efa0a44482010f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
