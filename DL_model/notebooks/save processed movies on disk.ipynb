{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628be7f8-32a6-4b00-8d25-b0113b344742",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "517407ac",
   "metadata": {},
   "source": [
    "# Process U-Net data and save on disk\n",
    "\n",
    "Use this script to save processed version of the samples in the dataset, their annotations, or their predictions on disk.\n",
    "\n",
    "Author: Prisca Dotti  \n",
    "Last modified: 02.11.2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ea84405-7f9b-417b-b8b1-a57fe34f6bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# autoreload is used to reload modules automatically before entering the\n",
    "# execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# To import modules from parent directory in Jupyter Notebook\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "790cf70f-20bc-4de9-808c-bf3963de199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import imageio\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from config import TrainingConfig, config\n",
    "from data.data_processing_tools import preds_dict_to_mask, process_raw_predictions\n",
    "from utils.in_out_tools import write_colored_events_videos_on_disk\n",
    "from utils.training_inference_tools import do_inference\n",
    "from utils.training_script_utils import init_dataset, init_model\n",
    "from utils.visualization_tools import (\n",
    "    add_colored_segmentation_to_video,\n",
    "    get_annotations_contour,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1b52ff-59e0-4cd6-a817-0ea04caf4a9f",
   "metadata": {},
   "source": [
    "### Load movies and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a1f9439-f9b4-47a8-b2fd-2936095ed201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_ids = [\"05\", \"10\", \"15\", \"20\", \"25\", \"32\", \"34\", \"40\", \"45\"]\n",
    "# movie_ids = [\"05\", \"34\"]\n",
    "movie_ids = [\"05\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "67be34ef-3900-450e-b298-34fcb6650c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:21:45] [  INFO  ] [   config   ] <290 > -- Loading C:\\Users\\prisc\\Code\\sparks_project\\config_files\\config_final_model.ini\n"
     ]
    }
   ],
   "source": [
    "training_name = \"final_model\"\n",
    "config_filename = \"config_final_model.ini\"\n",
    "load_epoch = 100000\n",
    "\n",
    "# Initialize general parameters\n",
    "params = TrainingConfig(\n",
    "    training_config_file=os.path.join(\"config_files\", config_filename)\n",
    ")\n",
    "params.run_name = training_name\n",
    "# params.set_device(device=\"auto\")\n",
    "params.set_device(device=\"cpu\")\n",
    "model_filename = f\"network_{load_epoch:06d}.pth\"\n",
    "\n",
    "# Output directory\n",
    "out_dir = os.path.join(\n",
    "    config.basedir, \"evaluation\", \"processed_movies_script\", training_name\n",
    ")\n",
    "os.makedirs(out_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af2a2c48-5b36-4f81-9d7b-71e19ccc63a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:21:56] [  INFO  ] [utils.training_script_utils] <137 > -- Samples in dataset: 9\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "dataset = init_dataset(\n",
    "    params=params,\n",
    "    sample_ids=movie_ids,\n",
    "    apply_data_augmentation=False,\n",
    "    print_dataset_info=True,\n",
    "    load_instances=True,\n",
    ")\n",
    "\n",
    "# Create a dataloader\n",
    "dataset_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=params.inference_batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=params.num_workers,\n",
    "    pin_memory=params.pin_memory,\n",
    ")\n",
    "\n",
    "xs = dataset.get_movies()\n",
    "ys = dataset.get_labels()\n",
    "ys_instances = dataset.get_instances()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ffa120",
   "metadata": {},
   "source": [
    "### Configure and load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "197ffd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure model architecture\n",
    "network = init_model(params=params)\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "if params.device.type != \"cpu\":\n",
    "    network = nn.DataParallel(network).to(params.device, non_blocking=True)\n",
    "    # cudnn.benchmark = True\n",
    "\n",
    "# Load model weights\n",
    "models_relative_path = os.path.join(\n",
    "    \"models\", \"saved_models\", params.run_name, model_filename\n",
    ")\n",
    "model_dir = os.path.realpath(os.path.join(config.basedir, models_relative_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a609b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model 'final_model' at epoch 100000...\n",
      "Failed to load the model, as it was trained with DataParallel. Wrapping it in DataParallel and retrying...\n",
      "Network should be on CPU, removing DataParallel wrapper...\n"
     ]
    }
   ],
   "source": [
    "# Load the model state dictionary\n",
    "print(f\"Loading trained model '{params.run_name}' at epoch {load_epoch}...\")\n",
    "try:\n",
    "    network.load_state_dict(torch.load(model_dir, map_location=params.device))\n",
    "except RuntimeError as e:\n",
    "    if \"module\" in str(e):\n",
    "        # The error message contains \"module,\" so handle the DataParallel loading\n",
    "        print(\n",
    "            \"Failed to load the model, as it was trained with DataParallel. Wrapping it in DataParallel and retrying...\"\n",
    "        )\n",
    "        # Get current device of the object (model)\n",
    "        temp_device = next(iter(network.parameters())).device\n",
    "\n",
    "        network = nn.DataParallel(network)\n",
    "        network.load_state_dict(torch.load(model_dir, map_location=params.device))\n",
    "\n",
    "        print(\"Network should be on CPU, removing DataParallel wrapper...\")\n",
    "        network = network.module.to(temp_device)\n",
    "    else:\n",
    "        # Handle other exceptions or re-raise the exception if it's unrelated\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beebe963",
   "metadata": {},
   "source": [
    "### Get U-Net's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c02b08c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get U-Net's raw predictions\n",
    "network.eval()\n",
    "raw_preds_inference = do_inference(\n",
    "    network=network,\n",
    "    params=params,\n",
    "    dataloader=dataset_loader,\n",
    "    device=params.device,\n",
    "    compute_loss=False,\n",
    "    inference_types=[params.inference],\n",
    ")\n",
    "\n",
    "# Remove middle dictionary from raw_preds\n",
    "raw_preds = {idx: pred[params.inference] for idx, pred in raw_preds_inference.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3807c613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get processed predictions\n",
    "final_segmentation_dict = {}\n",
    "final_instances_dict = {}\n",
    "sparks_coords_dict = {}\n",
    "\n",
    "for i in range(len(movie_ids)):\n",
    "    # transform raw predictions into a dictionary\n",
    "    raw_preds_dict = {\n",
    "        event_type: raw_preds[i][event_label]\n",
    "        for event_type, event_label in config.classes_dict.items()\n",
    "        if event_type in config.event_types\n",
    "    }\n",
    "\n",
    "    pred_instances, pred_segmentation, sparks_coords = process_raw_predictions(\n",
    "        raw_preds_dict=raw_preds_dict,\n",
    "        input_movie=xs[i],\n",
    "        training_mode=False,\n",
    "        debug=False,\n",
    "    )\n",
    "\n",
    "    final_segmentation_dict[movie_ids[i]] = pred_segmentation\n",
    "    final_instances_dict[movie_ids[i]] = pred_instances\n",
    "    sparks_coords_dict[movie_ids[i]] = sparks_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f26b1",
   "metadata": {},
   "source": [
    "## Paste colored segmentation on original movie\n",
    "Use this section to add colored annotations and predictions on given dataset movies.  \n",
    "Used to generate examples for midterm exams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dd8238b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video 05 ...\n",
      "Saving predicted segmentation masks on disk...\n",
      "Saving labelled segmentation masks on disk...\n",
      "Saving predicted instances masks on disk...\n",
      "Saving labelled instances masks on disk...\n",
      "Saving movies where annotations are denoted by border and preds are transparent...\n",
      "Saving predicted segmentation masks on white background...\n",
      "Saving labelled segmentation masks on white background...\n"
     ]
    }
   ],
   "source": [
    "segmentation_transparency = 70\n",
    "instances_transparency = 90\n",
    "\n",
    "\n",
    "for i, movie_id in enumerate(movie_ids):\n",
    "    print(\"Processing video\", movie_id, \"...\")\n",
    "    # Get processed predictions as numpy arrays of integers\n",
    "    pred_segmentation = preds_dict_to_mask(final_segmentation_dict[movie_id])\n",
    "    pred_instances = np.array(sum(final_instances_dict[movie_id].values()))\n",
    "\n",
    "    base_fn = f\"{training_name}_{load_epoch:06d}_{movie_id}\"\n",
    "\n",
    "    print(\"Saving predicted segmentation masks on disk...\")\n",
    "    write_colored_events_videos_on_disk(\n",
    "        movie=xs[i],\n",
    "        events_mask=pred_segmentation,\n",
    "        out_dir=out_dir,\n",
    "        movie_fn=f\"{base_fn}_colored_classes_preds\",\n",
    "        transparency=segmentation_transparency,\n",
    "        ignore_frames=0,\n",
    "        white_bg=False,\n",
    "        instances=False,\n",
    "    )\n",
    "\n",
    "    print(\"Saving labelled segmentation masks on disk...\")\n",
    "    write_colored_events_videos_on_disk(\n",
    "        movie=xs[i],\n",
    "        events_mask=ys[i],\n",
    "        out_dir=out_dir,\n",
    "        movie_fn=f\"{base_fn}_colored_classes_labels\",\n",
    "        transparency=segmentation_transparency,\n",
    "        ignore_frames=0,\n",
    "        white_bg=False,\n",
    "        instances=False,\n",
    "    )\n",
    "\n",
    "    # Add transparent instances masks to input movie and save on disk\n",
    "\n",
    "    print(\"Saving predicted instances masks on disk...\")\n",
    "    write_colored_events_videos_on_disk(\n",
    "        movie=xs[i],\n",
    "        events_mask=pred_instances,\n",
    "        out_dir=out_dir,\n",
    "        movie_fn=f\"{base_fn}_colored_instances_preds\",\n",
    "        transparency=instances_transparency,\n",
    "        ignore_frames=0,\n",
    "        white_bg=False,\n",
    "        instances=True,\n",
    "    )\n",
    "\n",
    "    print(\"Saving labelled instances masks on disk...\")\n",
    "    write_colored_events_videos_on_disk(\n",
    "        movie=xs[i],\n",
    "        events_mask=ys_instances[i],\n",
    "        out_dir=out_dir,\n",
    "        movie_fn=f\"{base_fn}_colored_instances_labels\",\n",
    "        transparency=segmentation_transparency,\n",
    "        ignore_frames=0,\n",
    "        white_bg=False,\n",
    "        instances=True,\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Saving movies where annotations are denoted by border and preds are transparent...\"\n",
    "    )\n",
    "    write_colored_events_videos_on_disk(\n",
    "        movie=xs[i],\n",
    "        events_mask=pred_segmentation,\n",
    "        out_dir=out_dir,\n",
    "        movie_fn=f\"{base_fn}_colored_preds_and_labels\",\n",
    "        transparency=segmentation_transparency,\n",
    "        ignore_frames=params.ignore_frames_loss,\n",
    "        white_bg=False,\n",
    "        instances=False,\n",
    "        label_mask=ys[i],\n",
    "    )\n",
    "\n",
    "    print(\"Saving predicted segmentation masks on white background...\")\n",
    "    write_colored_events_videos_on_disk(\n",
    "        movie=xs[i],\n",
    "        events_mask=pred_segmentation,\n",
    "        out_dir=out_dir,\n",
    "        movie_fn=f\"{base_fn}_colored_preds_white_bg\",\n",
    "        transparency=segmentation_transparency,\n",
    "        ignore_frames=params.ignore_frames_loss,\n",
    "        white_bg=True,\n",
    "    )\n",
    "\n",
    "    print(\"Saving labelled segmentation masks on white background...\")\n",
    "    write_colored_events_videos_on_disk(\n",
    "        movie=xs[i],\n",
    "        events_mask=ys[i],\n",
    "        out_dir=out_dir,\n",
    "        movie_fn=f\"{base_fn}_colored_labels_white_bg\",\n",
    "        transparency=segmentation_transparency,\n",
    "        ignore_frames=params.ignore_frames_loss,\n",
    "        white_bg=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035a7c4",
   "metadata": {},
   "source": [
    "### Save composed movies with segmentation masks and instance masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e06226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe0482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, movie_id in enumerate(movie_ids):\n",
    "    # Get processed predictions as numpy arrays of integers\n",
    "    pred_segmentation = preds_dict_to_mask(final_segmentation_dict[movie_id])\n",
    "    pred_instances = np.array(sum(final_instances_dict[movie_id].values()))\n",
    "\n",
    "    segmentation_transparency = 70\n",
    "    instances_transparency = 90\n",
    "    movie_rgb = np.copy(255 * (xs[i] / xs[i].max()))\n",
    "    movie_rgb = [Image.fromarray(frame).convert(\"RGB\") for frame in movie_rgb]\n",
    "\n",
    "    classes_dict = {  # Dataset parameters\n",
    "        \"sparks\": {\"nb\": 1, \"color\": [178, 255, 102]},  # Green\n",
    "        \"puffs\": {\"nb\": 3, \"color\": [255, 102, 102]},  # Red\n",
    "        \"waves\": {\"nb\": 2, \"color\": [178, 102, 255]},  # Purple\n",
    "        \"ignore\": {\"nb\": 4, \"color\": [224, 224, 224]},  # Gray\n",
    "    }\n",
    "\n",
    "    # Create movie with colored segmentation masks\n",
    "    segmentation_rgb = np.copy(255 * (xs[i] / xs[i].max()))\n",
    "    segmentation_rgb = [\n",
    "        Image.fromarray(frame).convert(\"RGB\") for frame in segmentation_rgb\n",
    "    ]\n",
    "\n",
    "    for class_info in classes_dict.values():\n",
    "        class_nb = class_info[\"nb\"]\n",
    "        color = class_info[\"color\"]\n",
    "\n",
    "        # Add colored predicted segmentation mask\n",
    "        if class_nb in pred_segmentation:\n",
    "            binary_preds = pred_segmentation == class_nb\n",
    "            segmentation_rgb = add_colored_segmentation_to_video(\n",
    "                segmentation=binary_preds,\n",
    "                video=segmentation_rgb,\n",
    "                color=color,\n",
    "                transparency=segmentation_transparency,\n",
    "            )\n",
    "\n",
    "        # Add annotated label contours\n",
    "        label_contours = get_annotations_contour(annotations=ys[i], contour_val=2)\n",
    "        if class_nb in ys[i]:\n",
    "            binary_labels = label_contours == class_nb\n",
    "            segmentation_rgb = add_colored_segmentation_to_video(\n",
    "                segmentation=binary_labels,\n",
    "                video=segmentation_rgb,\n",
    "                color=color,\n",
    "                transparency=1000,\n",
    "            )\n",
    "\n",
    "    # Create movie with colored annotated instances\n",
    "    y_instances_rgb = np.copy(255 * (xs[i] / xs[i].max()))\n",
    "    y_instances_rgb = [\n",
    "        Image.fromarray(frame).convert(\"RGB\") for frame in y_instances_rgb\n",
    "    ]\n",
    "\n",
    "    for event_id in range(1, ys_instances[i].max() + 1):\n",
    "        event_mask = ys_instances[i] == event_id\n",
    "\n",
    "        # Create a random color for each event\n",
    "        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "        y_instances_rgb = add_colored_segmentation_to_video(\n",
    "            segmentation=event_mask,\n",
    "            video=y_instances_rgb,\n",
    "            color=color,\n",
    "            transparency=instances_transparency,\n",
    "        )\n",
    "\n",
    "    # Create movie with colored predicted instances\n",
    "    pred_instances_rgb = np.copy(255 * (xs[i] / xs[i].max()))\n",
    "    pred_instances_rgb = [\n",
    "        Image.fromarray(frame).convert(\"RGB\") for frame in pred_instances_rgb\n",
    "    ]\n",
    "\n",
    "    for event_id in range(1, pred_instances.max() + 1):\n",
    "        event_mask = pred_instances == event_id\n",
    "\n",
    "        # Create a random color for each event\n",
    "        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "\n",
    "        pred_instances_rgb = add_colored_segmentation_to_video(\n",
    "            segmentation=event_mask,\n",
    "            video=pred_instances_rgb,\n",
    "            color=color,\n",
    "            transparency=instances_transparency,\n",
    "        )\n",
    "\n",
    "    # Concatenate movies vertically adding some space in between\n",
    "    movie_duration = len(movie_rgb)\n",
    "    space = 10\n",
    "\n",
    "    stacked_video = []\n",
    "    for frame in range(movie_duration):\n",
    "        result = Image.new(\n",
    "            \"RGB\",\n",
    "            (\n",
    "                movie_rgb[0].width,\n",
    "                4 * movie_rgb[0].height + 4 * space + 4 * 32,\n",
    "            ),\n",
    "            (255, 255, 255),\n",
    "        )\n",
    "        y_offset = 0\n",
    "\n",
    "        # Add text to the top of the frame\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 24)\n",
    "        draw = ImageDraw.Draw(result)\n",
    "        draw.text((10, y_offset), \"Original movie\", fill=(0, 0, 0), font=font)\n",
    "        y_offset += 32\n",
    "\n",
    "        # Add the original movie frame\n",
    "        result.paste(movie_rgb[frame], (0, y_offset))\n",
    "        y_offset += movie_rgb[frame].size[1] + space\n",
    "\n",
    "        # Add text above the annotated instances mask\n",
    "        draw.text((10, y_offset), \"Annotated instances\", fill=(0, 0, 0), font=font)\n",
    "        y_offset += 32\n",
    "\n",
    "        # Add the annotated instances mask\n",
    "        result.paste(y_instances_rgb[frame], (0, y_offset))\n",
    "        y_offset += y_instances_rgb[frame].size[1] + space\n",
    "\n",
    "        # Add text above the annotated segmentation mask\n",
    "        draw.text(\n",
    "            (10, y_offset),\n",
    "            \"Annotated vs. predicted segmentation masks\",\n",
    "            fill=(0, 0, 0),\n",
    "            font=font,\n",
    "        )\n",
    "        y_offset += 32\n",
    "\n",
    "        # Add the annotated segmentation mask\n",
    "        result.paste(segmentation_rgb[frame], (0, y_offset))\n",
    "        y_offset += segmentation_rgb[frame].size[1] + space\n",
    "\n",
    "        # Add text above the predicted instances mask\n",
    "        draw.text((10, y_offset), \"Predicted instances\", fill=(0, 0, 0), font=font)\n",
    "        y_offset += 32\n",
    "\n",
    "        # Add the predicted instances mask\n",
    "        result.paste(pred_instances_rgb[frame], (0, y_offset))\n",
    "        y_offset += pred_instances_rgb[frame].size[1] + space\n",
    "\n",
    "        stacked_video.append(result)\n",
    "\n",
    "    stacked_video = np.stack([np.array(frame) for frame in stacked_video])\n",
    "    movie_fn = f\"{training_name}_{load_epoch:06d}_{movie_id}_stacked_colored_preds_and_labels.tif\"\n",
    "    imageio.volwrite(os.path.join(out_dir, movie_fn), stacked_video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
