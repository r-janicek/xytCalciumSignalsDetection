; last training: 08/04/2023
[general]
; comment to disable
;wandb_enable = yes
wandb_notes = train final model again because lovasz softmax loss was lacking softmax activation, using log_softmax
; if training == 0 only run validation once:
training = yes
testing = yes
;debug_mode = yes


[training]
run_name = final_model
;run_name = final_model_log_softmax <- other run with same config file
;load_run_name = TEST
load_epoch = 100000
train_epochs = 90000
;criterion = nll_loss
;criterion = focal_loss
criterion = lovasz_softmax
lr_start = 1e-4
ignore_frames_loss = 6
; only for focal loss and summed losses:
;gamma = 5
; only for summed losses:
;w = 0.5
cuda = yes

save_every = 10000
test_every = 5000
print_every = 1000


[dataset]
relative_path = data/sparks_dataset
;dataset_size = full
dataset_size = minimal
;batch_size = 2
batch_size = 4
; num_workers = 0
data_duration = 256
data_stride = 32
data_smoothing = no
norm_video = abs_max
;norm_video = chunk
;remove_background = moving
remove_background = no
;only_sparks = no
;noise_data_augmentation = no
sparks_type = raw
;sparks_type = peaks


[network]
nn_architecture = pablos_unet
;nn_architecture = github_unet
unet_steps = 6
first_layer_channels = 8
;temporal_reduction = no
;num_channels = 1
;dilation = 1
border_mode = same
;batch_normalization = batch
batch_normalization = none
initialize_weights = no
attention = no
up_mode = transpose


[inference]
; use this params to validate the unet
data_duration = 256
data_stride = 32
inference = overlap
load_epoch = 100000
batch_size = 2
dataset_size = full
; dataset_size = minimal