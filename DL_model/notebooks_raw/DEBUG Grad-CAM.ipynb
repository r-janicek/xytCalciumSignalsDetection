{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.11.2022\n",
    "\n",
    "# Test Grad-CAM on UNet model\n",
    "\n",
    "Provo a usare una Grad-CAM su un modello salvato della UNet e il movie 34 (dove la fine della wave viene detettata come puff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload is used to reload modules automatically before entering the\n",
    "# execution of code typed at the IPython prompt.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# To import modules from parent directory in Jupyter Notebook\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "# from in_out_tools import write_videos_on_disk\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.visualization_tools import get_discrete_cmap\n",
    "\n",
    "from medcam import medcam\n",
    "import napari\n",
    "from utils.training_inference_tools import get_half_overlap\n",
    "from utils.training_script_utils import init_model\n",
    "from data.datasets import SparkDatasetInference\n",
    "\n",
    "from config import TrainingConfig, config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training 'final_model'...\n",
      "[23:05:48] [  INFO  ] [   config   ] <290 > -- Loading C:\\Users\\prisc\\Code\\sparks_project\\config_files\\config_final_model.ini\n"
     ]
    }
   ],
   "source": [
    "training_name = \"final_model\"\n",
    "config_file = \"config_final_model.ini\"\n",
    "\n",
    "print(f\"Processing training '{training_name}'...\")\n",
    "\n",
    "# Initialize general parameters\n",
    "params = TrainingConfig(training_config_file=os.path.join(\"config_files\", config_file))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure output folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files will be saved on 'C:\\Users\\prisc\\Code\\sparks_project\\evaluation\\gradCAM_script\\final_model'\n"
     ]
    }
   ],
   "source": [
    "# change this to save results for same model with different inference approaches\n",
    "output_name = training_name\n",
    "\n",
    "output_folder = os.path.join(\n",
    "    config.basedir, \"evaluation\", \"gradCAM_script\", output_name\n",
    ")  # Same folder for train and test preds\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "print(f\"Output files will be saved on '{os.path.realpath(output_folder)}'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect GPU, if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:05:49] [  INFO  ] [   config   ] <528 > -- Using cpu\n"
     ]
    }
   ],
   "source": [
    "params.set_device(device=\"auto\")\n",
    "params.display_device_info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = init_model(params=params)\n",
    "network = nn.DataParallel(network).to(params.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load UNet model ###\n",
    "# Path to the saved model checkpoint\n",
    "model_filename = f\"network_{params.inference_load_epoch:06d}.pth\"\n",
    "models_relative_path = os.path.join(\n",
    "    \"models\", \"saved_models\", params.run_name, model_filename\n",
    ")\n",
    "model_dir = os.path.realpath(os.path.join(config.basedir, models_relative_path))\n",
    "\n",
    "# Load the model state dictionary\n",
    "print(\n",
    "    f\"Loading trained model '{training_name}' at epoch {params.inference_load_epoch}...\"\n",
    ")\n",
    "network.load_state_dict(torch.load(model_dir, map_location=params.device))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print summary of network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary(network,  input_size=(1, 256, 64, 512), device=\"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load input sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dataset of movie C:\\Users\\prisc\\Code\\sparks_project\\data\\sparks_dataset\\05_class_label.tif contains 9 samples.\n"
     ]
    }
   ],
   "source": [
    "### Configure sample input ###\n",
    "movie_path = (\n",
    "    r\"C:\\Users\\prisc\\Code\\sparks_project\\data\\sparks_dataset\\05_class_label.tif\"\n",
    ")\n",
    "\n",
    "sample_dataset = SparkDatasetInference(\n",
    "    params=params,\n",
    "    movie_path=movie_path,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Testing dataset of movie {os.path.realpath(movie_path)} \"\n",
    "    f\"contains {len(sample_dataset)} samples.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader\n",
    "dataset_loader = DataLoader(\n",
    "    sample_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=params.num_workers,\n",
    "    pin_memory=params.pin_memory,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.eval()\n",
    "\n",
    "cam_network = medcam.inject(\n",
    "    network,\n",
    "    label=3,\n",
    "    replace=True,\n",
    "    # backend=\"gcam\",\n",
    "    layer=\"module.final_layer\",\n",
    "    output_dir=output_folder,\n",
    "    save_maps=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run sample's chunks in network and re-assemble UNet's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chunks = len(sample_dataset)\n",
    "half_overlap = get_half_overlap(\n",
    "    data_duration=params.data_duration,\n",
    "    data_stride=params.data_stride,\n",
    "    temporal_reduction=params.temporal_reduction,\n",
    "    num_channels=params.num_channels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network device cpu\n"
     ]
    }
   ],
   "source": [
    "# print(\"network device\", next(cam_network.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x device cpu\n",
      "network device cpu\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\prisc\\Code\\sparks_project\\notebooks\\DEBUG Grad-CAM.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prisc/Code/sparks_project/notebooks/DEBUG%20Grad-CAM.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mx device\u001b[39m\u001b[39m\"\u001b[39m, x\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prisc/Code/sparks_project/notebooks/DEBUG%20Grad-CAM.ipynb#X36sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mnetwork device\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mnext\u001b[39m(cam_network\u001b[39m.\u001b[39mparameters())\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/prisc/Code/sparks_project/notebooks/DEBUG%20Grad-CAM.ipynb#X36sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     out \u001b[39m=\u001b[39m cam_network(x[\u001b[39mNone\u001b[39;49;00m, :])[\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prisc/Code/sparks_project/notebooks/DEBUG%20Grad-CAM.ipynb#X36sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     out_concat\u001b[39m.\u001b[39mappend(out[start:end]\u001b[39m.\u001b[39mcpu())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/prisc/Code/sparks_project/notebooks/DEBUG%20Grad-CAM.ipynb#X36sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m x_concat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(x_concat, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\medcam\\medcam_inject.py:203\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, batch, label, mask, raw_input)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmedcam_dict[\u001b[39m'\u001b[39m\u001b[39menabled\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m    202\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 203\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtest_run(batch, internal\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    204\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmedcam_dict[\u001b[39m'\u001b[39m\u001b[39mlayer\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmedcam_dict[\u001b[39m'\u001b[39m\u001b[39mtested\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m    205\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLayer mode \u001b[39m\u001b[39m'\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m'\u001b[39m\u001b[39m requires a test run either during injection or by calling test_run() afterwards\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\medcam\\medcam_inject.py:232\u001b[0m, in \u001b[0;36mtest_run\u001b[1;34m(self, batch, internal)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m batch \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmedcam_dict[\u001b[39m'\u001b[39m\u001b[39mtested\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m    231\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 232\u001b[0m         _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmedcam_dict[\u001b[39m'\u001b[39;49m\u001b[39mmodel_backend\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mgenerate_attention_map(batch, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    233\u001b[0m         registered_hooks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmedcam_dict[\u001b[39m'\u001b[39m\u001b[39mmodel_backend\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mget_registered_hooks()\n\u001b[0;32m    234\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmedcam_dict[\u001b[39m'\u001b[39m\u001b[39mtested\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\medcam\\backends\\base.py:21\u001b[0m, in \u001b[0;36m_BaseWrapper.generate_attention_map\u001b[1;34m(self, batch, label)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_attention_map\u001b[39m(\u001b[39mself\u001b[39m, batch, label):\n\u001b[0;32m     20\u001b[0m     \u001b[39m\"\"\"Handles the generation of the attention map from start to finish.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(batch)\n\u001b[0;32m     22\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward(label\u001b[39m=\u001b[39mlabel)\n\u001b[0;32m     23\u001b[0m     attention_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate()\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\medcam\\backends\\grad_cam.py:110\u001b[0m, in \u001b[0;36mGradCAM.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39m\"\"\"Calls the forward() of the base.\"\"\"\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_register_hooks()\n\u001b[1;32m--> 110\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(GradCAM, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mforward(data)\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\medcam\\backends\\base.py:29\u001b[0m, in \u001b[0;36m_BaseWrapper.forward\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39m\"\"\"Calls the forward() of the model.\"\"\"\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 29\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mmodel_forward(batch)\n\u001b[0;32m     30\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extract_metadata(batch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits)\n\u001b[0;32m     31\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_postprocessor_and_label(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits)\n",
      "File \u001b[1;32mc:\\Users\\prisc\\anaconda3\\envs\\sparks\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:157\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m chain(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mbuffers()):\n\u001b[0;32m    156\u001b[0m     \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mdevice \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj:\n\u001b[1;32m--> 157\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule must have its parameters and buffers \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mon device \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m (device_ids[0]) but found one of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mthem on device: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_device_obj, t\u001b[39m.\u001b[39mdevice))\n\u001b[0;32m    161\u001b[0m inputs, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscatter(inputs, kwargs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids)\n\u001b[0;32m    162\u001b[0m \u001b[39m# for forward function without any inputs, empty list and dict will be created\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39m# so the module can be executed on one device which is the first one in device_ids\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cpu"
     ]
    }
   ],
   "source": [
    "out_concat = []\n",
    "x_concat = []\n",
    "\n",
    "cam_network.eval()\n",
    "for i, sample in enumerate(dataset_loader):\n",
    "    x = sample[\"data\"]\n",
    "\n",
    "    # define start and end of used frames in chunks\n",
    "    start = 0 if i == 0 else half_overlap\n",
    "    end = None if i + 1 == n_chunks else -half_overlap\n",
    "\n",
    "    x_concat.append(x[0, start:end])\n",
    "\n",
    "    x = x.to(params.device)\n",
    "    print(\"x device\", x.device)\n",
    "    print(\"network device\", next(cam_network.parameters()).device)\n",
    "    out = cam_network(x[None, :])[0, 0]\n",
    "    out_concat.append(out[start:end].cpu())\n",
    "x_concat = torch.cat(x_concat, dim=0).numpy()\n",
    "out_concat = torch.cat(out_concat, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(928, 64, 512)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_concat.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise result with Napari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Napari cmap\n",
    "cmap = get_discrete_cmap(name=\"gray\", lut=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'network output' at 0x14486fb6130>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer = napari.Viewer()\n",
    "\n",
    "viewer.add_image(x_concat, name=\"input movie\", colormap=(\"colors\", cmap))\n",
    "\n",
    "viewer.add_image(out_concat, name=\"network output\", colormap=(\"colors\", cmap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d21ea8e127dc18603ed933cb69d3f0e3fbc5bcbc2dd19f44b0f9b09cfcc47615"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
